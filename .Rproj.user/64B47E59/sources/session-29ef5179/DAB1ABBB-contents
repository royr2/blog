---
title: "Optimising Approval Rates for Credit Applications"
date: "2025-03-30"
categories: [R, Credit Risk Analytics, Optimization]
image: "../images/approval_rates.png"
execute:
  echo: true
  warning: false
  message: false
  eval: true
---

# Optimising Approval Rates with Segmented Models

In credit risk modeling, finding the right balance between approval rates and risk is a constant challenge. Financial institutions aim to approve as many creditworthy applicants as possible while maintaining acceptable risk levels. This post demonstrates how developing segmented models can significantly improve this balance compared to using a single population-wide model.

While segmentation offers many benefits in credit risk modeling, this post focuses specifically on how it can optimize approval rates without increasing risk exposure. I'll show a practical example using lending data to illustrate the concept.

The sample dataset used in this analysis is available for download from [GitHub](https://github.com/royr2/blog/blob/main/download/credit_sample.csv), and I'll be building on techniques covered in my previous post on gains tables.

## Required Packages

First, let's load the R packages we'll need for our analysis:

```{r}
#| label: setup

# Load required packages
library(dplyr)     # For data manipulation
library(magrittr)  # For pipe operators
library(scales)    # For formatting percentages in plots
library(pROC)      # For ROC curve analysis
```

## Sample Data Preparation

Next, we'll load and prepare our sample dataset, which contains loan application information:

```{r}
#| label: data-prep

# Load sample loan data from GitHub
smple <- read.csv("https://bit.ly/42ypcnJ")

# Define target variable (1 = bad loan, 0 = good loan)
# Bad loans are those that were charged off or failed to meet credit policy
codes <- c("Charged Off", "Does not meet the credit policy. Status:Charged Off")
smple %<>% mutate(bad_flag = ifelse(loan_status %in% codes, 1, 0))

# Basic data cleaning steps
# Replace missing values with -1 (a common practice in credit scoring)
smple[is.na(smple)] <- -1

# Remove records with missing or invalid categorical values
smple %<>% 
  filter(!home_ownership %in% c("", "NONE"),  # Remove missing home ownership
         pymnt_plan != "") %>%                 # Remove missing payment plan
  
  # Convert categorical variables to factors for modeling
  mutate(home_ownership = factor(home_ownership), 
         pymnt_plan = factor(pymnt_plan))
```

## Creating Segments

In credit risk modeling, segmentation is a common practice to group customers with similar risk profiles. Typical segments might include "known goods," "known bads," "thin files," or "new-to-credit" customers.

For this demonstration, we'll use the FICO score (a widely used credit score in the US) to create simple segments:

```{r}
#| label: segmentation

# Create segments based on FICO score ranges
smple %<>%
  mutate(segment = case_when(
    fico_range_low >= 700 ~ "KG",  # Known Good (high FICO score)
    fico_range_low <= 600 ~ "KB",  # Known Bad (low FICO score)
    TRUE ~ "Others"                # Middle range FICO scores
  ))

# Check the distribution of records across segments
table(smple$segment)
```

For this analysis, we'll focus on the "Known Good" (KG) segment. In practice, most lenders would apply policy filters to automatically reject applicants with very poor credit histories (the KB segment), so optimizing the approval strategy for the better-quality segments often yields the most business value.

## Building Models

To demonstrate the value of segmentation, we'll build two logistic regression models:

1. **Population model**: Built using all available data
2. **Segment model**: Built specifically for the "Known Good" segment

Both models will use the same set of predictors, but the segment model will be trained only on data from the KG segment:

```{r}
#| label: model-building

# Build a population-wide model using all data
mdl_pop <- glm(bad_flag ~ annual_inc + dti + home_ownership + purpose + term, 
               data = smple,                  # Using all data
               family = "binomial")           # Logistic regression

# Build a segment-specific model for Known Good customers only
mdl_seg <- glm(bad_flag ~ annual_inc + dti + home_ownership + purpose + term, 
               data = filter(smple, segment == "KG"),  # Using only KG segment
               family = "binomial") 

# Define a function to convert log-odds to credit score range (300-850)
# This makes the model outputs more interpretable
scaling_func <- function(x, min_score = 300, max_score = 850) {
  # Transform log-odds to probability, then scale to credit score range
  # Higher scores indicate lower risk of default
  scaled_score <- min_score + (max_score - min_score) * 
                  (1 - (1 / (1 + exp(-x))))
  return(scaled_score)
}
```

The models use several common credit risk predictors:
- `annual_inc`: Annual income
- `dti`: Debt-to-income ratio
- `home_ownership`: Housing status (own, rent, etc.)
- `purpose`: Loan purpose
- `term`: Loan term length

## Comparing Model Performance

To evaluate the benefit of segmentation, we'll compare how both models perform specifically on the "Known Good" segment. The key question is: Can a segment-specific model approve more customers at the same risk level compared to a population-wide model?

We'll calculate performance metrics at different approval rate thresholds:

```{r}
#| label: model-comparison

# Evaluate population model performance on the KG segment
pop_perf <- smple %>%
  filter(segment == "KG") %>%                     # Focus on KG segment only
  mutate(pred = predict(mdl_pop, newdata = .),   # Get model predictions
         score = scaling_func(pred),             # Convert to credit score scale
         total = n()) %>%                         # Total number of KG customers
  arrange(desc(score)) %>%                        # Sort by score (best first)
  mutate(cum_count = row_number(),               # Cumulative count
         cum_pct = round(cum_count / total, 2),            # Approval rate
         cum_bad = cumsum(bad_flag),             # Cumulative bad loans
         cum_bad_rate = cum_bad / cum_count) %>% # Bad rate at each approval threshold
  filter(cum_pct %in% seq(0.1, 1, 0.1))          # Sample at 10% intervals

# Evaluate segment model performance on the KG segment
seg_perf <- smple %>%
  filter(segment == "KG") %>%                     # Same segment
  mutate(pred = predict(mdl_seg, newdata = .),   # But using segment model
         score = scaling_func(pred),
         total = n()) %>%
  arrange(desc(score)) %>%
  mutate(cum_count = row_number(),
         cum_pct = round(cum_count / total, 2),
         cum_bad = cumsum(bad_flag),
         cum_bad_rate = cum_bad / cum_count) %>%
  filter(cum_pct %in% seq(0.1, 1, 0.1))
```

This analysis creates performance tables for both models at different approval rate thresholds (10%, 20%, etc.). For each threshold, we calculate the corresponding bad rate - the percentage of approved applications that would result in default.

## Visualizing the Results

A visual comparison helps us clearly see the difference in performance between the two models:

```{r}
#| label: visualization

# Combine results from both models for plotting
pop_perf$model <- "Population Model"
seg_perf$model <- "Segment Model"
combined <- rbind(pop_perf, seg_perf)

# Create a line plot comparing bad rates at different approval thresholds
library(ggplot2)
ggplot(combined, aes(x = cum_pct, y = cum_bad_rate, color = model, group = model)) +
  geom_line(size = 1) +                                  # Connect points with lines
  geom_point(size = 3) +                                 # Add points at each threshold
  scale_x_continuous(labels = percent_format(),         # Format x-axis as percentages
                     breaks = seq(0, 1, 0.1)) +         # Show 10% increments
  scale_y_continuous(labels = percent_format()) +       # Format y-axis as percentages
  labs(x = "Approval Rate", 
       y = "Bad Rate",
       title = "Bad Rate by Approval Rate",
       subtitle = "Comparison of Population vs. Segment Model") +
  theme_minimal() +                                     # Clean visual theme
  theme(legend.title = element_blank(),                 # Remove legend title
        legend.position = "bottom")                      # Position legend at bottom
```

This plot shows the relationship between approval rates (x-axis) and bad rates (y-axis) for both models. Lower curves indicate better performance - the ability to approve more customers while maintaining lower bad rates.

## Key Findings

When we compare the performance of both models on the "Known Good" segment, several important insights emerge:

1. **Improved risk-approval trade-off**: The segment model typically shows a lower bad rate at the same approval rate compared to the population model.

2. **Business impact option 1 - Increase approvals**: Using the segment model, we can approve more customers while maintaining the same risk level. For example, if a lender has a maximum acceptable bad rate of 3%, the segment model might allow approving 70% of applicants versus only 60% with the population model.

3. **Business impact option 2 - Reduce risk**: Alternatively, we can maintain the same approval rate but reduce the overall bad rate. This translates to fewer defaults and lower credit losses.

4. **Better targeting**: The segment model better captures the specific risk factors relevant to the "Known Good" population, rather than being influenced by patterns in the overall population.

## Why Segmentation Works

Segmentation improves model performance for several technical and business reasons:

1. **Different risk drivers**: Different customer segments often have fundamentally different risk drivers. For example, income might be highly predictive for new borrowers but less important for those with established credit histories. A segment-specific model can focus on the variables most relevant to that particular group.

2. **Coefficient stability**: Even when the same variables are predictive across segments, their effect size (coefficients) can vary significantly. For instance, a high debt-to-income ratio might indicate moderate risk for homeowners but severe risk for renters. Segment models capture these nuanced relationships.

3. **Improved discrimination**: By focusing on a more homogeneous population, segment-specific models can better distinguish between good and bad customers within that segment. This is particularly valuable in segments where the overall default rate is low, as in our "Known Good" example.

4. **Business alignment**: Segments often naturally align with business processes, product offerings, or customer journeys. This makes segment-specific models easier to implement and explain to stakeholders.

5. **Regulatory considerations**: In some jurisdictions, segmented models may help demonstrate fair lending practices by showing that different customer groups are evaluated using appropriate criteria.

## Practical Implementation Considerations

When implementing segmented models in a production environment, consider these best practices:

1. **Segment definition**: Create segments that are stable, meaningful, and large enough to build robust models

2. **Validation**: Thoroughly validate each segment model separately, as well as the overall strategy

3. **Monitoring**: Implement segment-specific monitoring to detect population shifts or model degradation

4. **Fallback strategy**: Develop a fallback approach for applications that don't clearly fit into defined segments

## Conclusion

Building segment-specific models can significantly improve approval rates while maintaining acceptable risk levels. In the competitive lending landscape, this approach provides a meaningful advantage by better aligning risk assessment with customer characteristics.

The example in this post demonstrates how a segment-specific model for "Known Good" customers can approve more applicants at the same risk level compared to a population-wide model. This translates directly to business value: more customers, higher revenue, and potentially lower credit losses.

For organizations looking to optimize their credit risk strategies, segmentation should be a key consideration in the modeling approach. The additional complexity is well justified by the improved performance and business outcomes.
