left_join(ticker_list %>%
mutate(ticker = paste0(Symbol, ".NS")) %>%
select(ticker, industry = Industry),
by = "ticker") %>%
# Convert date strings to Date objects
mutate(date = as.Date(date)) %>%
# Filter to show only metal industry stocks for clarity
filter(stringr::str_detect(tolower(industry), "metal")) %>%
# Create the line plot
ggplot(aes(x = date, y = price, color = ticker)) +
geom_line(linewidth = 0.8) +
theme_minimal() +
scale_color_brewer(palette = "RdBu") +  # Use a color-blind friendly palette
labs(title = "Closing Prices",
subtitle = "Nifty 50 metal stocks",
x = "Date",
y = "Closing Price") +
theme(legend.position = "top",
legend.title = element_text(colour = "transparent"),
axis.title.x = element_text(face = "bold"),
axis.title.y = element_text(face = "bold"))
prices_df
prices_df
prices_df
prices_df %>%
# Convert from wide to long format for easier plotting with ggplot2
pivot_longer(-date, names_to = "ticker")
prices_df %>%
# Convert from wide to long format for easier plotting with ggplot2
pivot_longer(-date, names_to = "ticker", values_to = "price")
# Plot closing prices for metal stocks
prices_df %>%
# Convert from wide to long format for easier plotting with ggplot2
pivot_longer(-date, names_to = "ticker", values_to = "price") %>%
# Attach industry information from our original ticker list
left_join(ticker_list %>%
mutate(ticker = paste0(Symbol, ".NS")) %>%
select(ticker, industry = Industry),
by = "ticker") %>%
# Convert date strings to Date objects
mutate(date = as.Date(date)) %>%
# Filter to show only metal industry stocks for clarity
filter(stringr::str_detect(tolower(industry), "metal")) %>%
# Create the line plot
ggplot(aes(x = date, y = price, color = ticker)) +
geom_line(linewidth = 0.8) +
theme_minimal() +
scale_color_brewer(palette = "RdBu") +  # Use a color-blind friendly palette
labs(title = "Closing Prices",
subtitle = "Nifty 50 metal stocks",
x = "Date",
y = "Closing Price") +
theme(legend.position = "top",
legend.title = element_text(colour = "transparent"),
axis.title.x = element_text(face = "bold"),
axis.title.y = element_text(face = "bold"))
# Calculate daily returns for all stocks
# Formula: (Price_today / Price_yesterday) - 1
returns_df <- apply(prices_df[,-1], 2, function(vec){
ret <- vec/lag(vec) - 1  # Simple returns calculation
return(ret)
})
# Convert to dataframe for easier manipulation
returns_df <- as.data.frame(returns_df)
# Remove first row which contains NA values (no previous day to calculate return)
returns_df <- returns_df[-1,]
# Pre-compute average returns and covariance matrix for optimization
# These are key inputs to the mean-variance optimization
mean_returns <- sapply(returns_df, mean)  # Expected returns
cov_mat <- cov(returns_df)  # Risk (covariance) matrix
obj_func <- function(wts,
risk_av = 10,  # Risk aversion parameter
lambda1 = 10,  # Penalty weight for full investment constraint
lambda2 = 1e2,  # Reserved for additional constraints
ret_vec, cov_mat){
# Calculate expected portfolio return (weighted average of asset returns)
port_returns <- ret_vec %*% wts
# Calculate portfolio risk (quadratic form using covariance matrix)
port_risk <- t(wts) %*% cov_mat %*% wts
# Mean-variance utility function: return - risk_aversion * risk
# This is the core Markowitz portfolio optimization formula
obj <- port_returns - risk_av * port_risk
# Add penalty for violating the full investment constraint (sum of weights = 1)
# The squared term ensures the penalty increases quadratically with violation size
obj <- obj - lambda1 * (sum(wts) - 1)^2
# Return negative value since PSO minimizes by default, but we want to maximize
# our objective (higher returns, lower risk)
return(-obj)
}
# Use only the first two assets for this example
# Calculate their average returns and covariance matrix
mean_returns_small <- apply(returns_df[,1:2], 2, mean)
cov_mat_small <- cov(returns_df[,1:2])
# Define a custom PSO optimizer function to track the optimization process
pso_optim <- function(obj_func,
c1 = 0.05,      # Cognitive parameter (personal best influence)
c2 = 0.05,      # Social parameter (global best influence)
w = 0.8,        # Inertia weight (controls momentum)
init_fact = 0.1, # Initial velocity factor
n_particles = 20, # Number of particles in the swarm
n_dim = 2,       # Dimensionality (number of assets)
n_iter = 50,     # Maximum iterations
upper = 1,       # Upper bound for weights
lower = 0,       # Lower bound for weights (no short selling)
n_avg = 10,      # Number of iterations for averaging
...){
# Initialize particle positions randomly within bounds
X <- matrix(runif(n_particles * n_dim), nrow = n_particles)
X <- X * (upper - lower) + lower  # Scale to fit within bounds
# Initialize particle velocities (movement speeds)
dX <- matrix(runif(n_particles * n_dim) * init_fact, ncol = n_dim)
dX <- dX * (upper - lower) + lower
# Initialize personal best positions and objective values
pbest <- X  # Each particle's best position so far
pbest_obj <- apply(X, 1, obj_func, ...)  # Objective value at personal best
# Initialize global best position and objective value
gbest <- pbest[which.min(pbest_obj),]  # Best position across all particles
gbest_obj <- min(pbest_obj)  # Best objective value found
# Store initial positions for visualization
loc_df <- data.frame(X, iter = 0, obj = pbest_obj)
iter <- 1
# Main PSO loop
while(iter < n_iter){
# Update velocities using PSO formula:
# New velocity = inertia + cognitive component + social component
dX <- w * dX +                         # Inertia (continue in same direction)
c1*runif(1)*(pbest - X) +        # Pull toward personal best
c2*runif(1)*t(gbest - t(X))      # Pull toward global best
# Update positions based on velocities
X <- X + dX
# Evaluate objective function at new positions
obj <- apply(X, 1, obj_func, ...)
# Update personal bests if new positions are better
idx <- which(obj <= pbest_obj)
pbest[idx,] <- X[idx,]
pbest_obj[idx] <- obj[idx]
# Update global best if a better solution is found
idx <- which.min(pbest_obj)
gbest <- pbest[idx,]
gbest_obj <- min(pbest_obj)
# Store current state for visualization
iter <- iter + 1
loc_df <- rbind(loc_df, data.frame(X, iter = iter, obj = pbest_obj))
}
# Return optimization results
lst <- list(X = loc_df,          # All particle positions throughout optimization
obj = gbest_obj,     # Best objective value found
obj_loc = gbest)     # Weights that achieved the best objective
return(lst)
}
# Run the optimization for our two-asset portfolio
out <- pso_optim(obj_func,
ret_vec = mean_returns_small,  # Expected returns
cov_mat = cov_mat_small,       # Covariance matrix
lambda1 = 10, risk_av = 100,    # Constraint and risk parameters
n_particles = 100,              # Use 100 particles for better coverage
n_dim = 2,                      # Two-asset portfolio
n_iter = 200,                   # Run for 200 iterations
upper = 1, lower = 0,           # Bounds for weights
c1 = 0.02, c2 = 0.02,           # Lower influence parameters for stability
w = 0.05, init_fact = 0.01)     # Low inertia for better convergence
# Verify that the weights sum to approximately 1 (full investment constraint)
sum(out$obj_loc)
# Create a fine grid of points covering the feasible region (all possible weight combinations)
grid <- expand.grid(x = seq(0, 1, by = 0.01),  # First asset weight from 0 to 1
y = seq(0, 1, by = 0.01))   # Second asset weight from 0 to 1
# Evaluate the objective function at each grid point to create the landscape
grid$obj <- apply(grid, 1, obj_func,
ret_vec = mean_returns_small,
cov_mat = cov_mat_small,
lambda1 = 10, risk_av = 100)
# Create an interactive 3D plot showing both the objective function surface
# and the particle trajectories throughout the optimization
p <- plot_ly() %>%
# Add the objective function surface as a mesh
add_mesh(data = grid, x = ~x, y = ~y, z = ~obj,
inherit = FALSE, color = "red") %>%
# Add particles as markers, colored by iteration to show progression
add_markers(data = out$X, x = ~X1, y = ~X2, z = ~obj,
color = ~ iter, inherit = FALSE,
marker = list(size = 2))
# Save the interactive plot as an HTML file
htmlwidgets::saveWidget(p, "plotly.html")
p
# Get the number of stocks in our dataset
n_stocks <- ncol(returns_df)
# Run the PSO optimization for the full portfolio
opt <- psoptim(
# Initial particle positions (starting with equal weights)
par = rep(0, n_stocks),
# Objective function to minimize
fn = obj_func,
# Pass the expected returns and covariance matrix
ret_vec = mean_returns,
cov_mat = cov_mat,
# Set constraint parameters
lambda1 = 10,  # Weight for full investment constraint
risk_av = 1000,  # Higher risk aversion for a more conservative portfolio
# Set bounds for weights (no short selling allowed)
lower = rep(0, n_stocks),
upper = rep(1, n_stocks),
# Configure the PSO algorithm
control = list(
maxit = 200,          # Maximum iterations
s = 100,               # Swarm size (number of particles)
maxit.stagnate = 500   # Stop if no improvement after this many iterations
)
)
# Calculate and display the expected return of the optimized portfolio
paste("Portfolio returns:", round(opt$par %*% mean_returns, 5))
# Calculate and display the standard deviation (risk) of the optimized portfolio
paste("Portfolio Std dev:", round(sqrt(opt$par %*% cov_mat %*% opt$par), 5))
# Verify that the weights sum to approximately 1 (full investment constraint)
sum(opt$par)
# Define benchmark portfolio (equally weighted across all stocks)
bench_wts <- rep(1/n_stocks, n_stocks)
# Calculate the time series of benchmark returns
bench_returns <- as.matrix(returns_df) %*% t(t(bench_wts))
# Create a new objective function that includes tracking error
obj_func_TE <- function(wts,
risk_av = 10,     # Risk aversion parameter
lambda1 = 10,    # Full investment constraint weight
lambda2 = 50,    # Tracking error constraint weight
ret_vec, cov_mat){
# Calculate portfolio metrics
port_returns <- ret_vec %*% wts                      # Expected portfolio return
port_risk <- t(wts) %*% cov_mat %*% wts             # Portfolio variance
port_returns_ts <- as.matrix(returns_df) %*% t(t(wts))  # Time series of portfolio returns
# Original mean-variance objective
obj <- port_returns - risk_av * port_risk
# Full investment constraint (weights sum to 1)
obj <- obj - lambda1 * (sum(wts) - 1)^2
# Tracking error constraint (penalize deviation from benchmark)
# Tracking error is measured as the standard deviation of the difference
# between portfolio returns and benchmark returns
obj <- obj - lambda2 * sd(port_returns_ts - bench_returns)
return(-obj)  # Return negative for minimization
}
# Run optimization with the tracking error constraint
opt <- psoptim(
# Initial particle positions
par = rep(0, n_stocks),
# Use our new objective function with tracking error
fn = obj_func_TE,
# Pass the expected returns and covariance matrix
ret_vec = mean_returns,
cov_mat = cov_mat,
# Set constraint parameters
lambda1 = 10,    # Weight for full investment constraint
risk_av = 1000,  # Risk aversion parameter
# Set bounds for weights
lower = rep(0, n_stocks),
upper = rep(1, n_stocks),
# Configure the PSO algorithm
control = list(
maxit = 200,          # Maximum iterations
s = 100,               # Swarm size
maxit.stagnate = 500   # Stop if no improvement after this many iterations
)
)
# Calculate and display the expected return of the optimized portfolio
paste("Portfolio returns:", round(opt$par %*% mean_returns, 5))
# Calculate and display the standard deviation (risk) of the optimized portfolio
paste("Portfolio Std dev:", round(sqrt(opt$par %*% cov_mat %*% opt$par), 5))
# Verify that the weights sum to approximately 1
sum(opt$par)
# Append ".NS" to tickers for Yahoo Finance format (NS = National Stock Exchange)
tickers <- paste0(ticker_list$Symbol, ".NS")
tickers <- tickers[!tickers %in% c("ETERNAL.NS", "JIOFIN.NS")]
# Initialize empty dataframe to store all ticker data
ticker_df <- data.frame()
# Create a progress bar to monitor the download process
# pb <- txtProgressBar(min = 1, max = length(tickers), style = 3)
# Loop through each ticker and download its historical data
for(nms in tickers){
# Download data from Yahoo Finance
df <- getSymbols(Symbols = nms, verbose = FALSE, auto.assign = FALSE)
# Rename columns for clarity
colnames(df) <- c("open", "high", "low", "close", "volume", "adjusted")
df$date = rownames(df)
# Convert to dataframe and add ticker and date information
df <- data.frame(df)
df$ticker <- nms
df$date <- rownames(df)
# Append to the main dataframe
ticker_df <- rbind(ticker_df, df)
Sys.sleep(0.2)
# Update progress bar
# setTxtProgressBar(pb, which(tickers == nms))
}
source("C:/Users/riddh/OneDrive/Desktop/rtichoke/install_packages.R")
#| label: setup
# Load required packages
library(dplyr)     # For data manipulation
library(magrittr)  # For pipe operators
library(scales)    # For formatting percentages in plots
library(pROC)      # For ROC curve analysis
#| label: data-prep
# Load sample loan data from GitHub
smple <- read.csv("https://bit.ly/42ypcnJ")
# Define target variable (1 = bad loan, 0 = good loan)
# Bad loans are those that were charged off or failed to meet credit policy
codes <- c("Charged Off", "Does not meet the credit policy. Status:Charged Off")
smple %<>% mutate(bad_flag = ifelse(loan_status %in% codes, 1, 0))
# Basic data cleaning steps
# Replace missing values with -1 (a common practice in credit scoring)
smple[is.na(smple)] <- -1
# Remove records with missing or invalid categorical values
smple %<>%
filter(!home_ownership %in% c("", "NONE"),  # Remove missing home ownership
pymnt_plan != "") %>%                 # Remove missing payment plan
# Convert categorical variables to factors for modeling
mutate(home_ownership = factor(home_ownership),
pymnt_plan = factor(pymnt_plan))
#| label: segmentation
# Create segments based on FICO score ranges
smple %<>%
mutate(segment = case_when(
fico_range_low >= 700 ~ "KG",  # Known Good (high FICO score)
fico_range_low <= 600 ~ "KB",  # Known Bad (low FICO score)
TRUE ~ "Others"                # Middle range FICO scores
))
# Check the distribution of records across segments
table(smple$segment)
#| label: model-building
# Build a population-wide model using all data
mdl_pop <- glm(bad_flag ~ annual_inc + dti + home_ownership + purpose + term,
data = smple,                  # Using all data
family = "binomial")           # Logistic regression
# Build a segment-specific model for Known Good customers only
mdl_seg <- glm(bad_flag ~ annual_inc + dti + home_ownership + purpose + term,
data = filter(smple, segment == "KG"),  # Using only KG segment
family = "binomial")
# Define a function to convert log-odds to credit score range (300-850)
# This makes the model outputs more interpretable
scaling_func <- function(x, min_score = 300, max_score = 850) {
# Transform log-odds to probability, then scale to credit score range
# Higher scores indicate lower risk of default
scaled_score <- min_score + (max_score - min_score) *
(1 - (1 / (1 + exp(-x))))
return(scaled_score)
}
#| label: model-comparison
# Evaluate population model performance on the KG segment
pop_perf <- smple %>%
filter(segment == "KG") %>%                     # Focus on KG segment only
mutate(pred = predict(mdl_pop, newdata = .),   # Get model predictions
score = scaling_func(pred),             # Convert to credit score scale
total = n()) %>%                         # Total number of KG customers
arrange(desc(score)) %>%                        # Sort by score (best first)
mutate(cum_count = row_number(),               # Cumulative count
cum_pct = cum_count / total,            # Approval rate
cum_bad = cumsum(bad_flag),             # Cumulative bad loans
cum_bad_rate = cum_bad / cum_count) %>% # Bad rate at each approval threshold
filter(cum_pct %in% seq(0.1, 1, 0.1))          # Sample at 10% intervals
# Evaluate segment model performance on the KG segment
seg_perf <- smple %>%
filter(segment == "KG") %>%                     # Same segment
mutate(pred = predict(mdl_seg, newdata = .),   # But using segment model
score = scaling_func(pred),
total = n()) %>%
arrange(desc(score)) %>%
mutate(cum_count = row_number(),
cum_pct = cum_count / total,
cum_bad = cumsum(bad_flag),
cum_bad_rate = cum_bad / cum_count) %>%
filter(cum_pct %in% seq(0.1, 1, 0.1))
#| label: visualization
# Combine results from both models for plotting
pop_perf$model <- "Population Model"
seg_perf$model <- "Segment Model"
combined <- rbind(pop_perf, seg_perf)
# Create a line plot comparing bad rates at different approval thresholds
library(ggplot2)
ggplot(combined, aes(x = cum_pct, y = cum_bad_rate, color = model, group = model)) +
geom_line(size = 1) +                                  # Connect points with lines
geom_point(size = 3) +                                 # Add points at each threshold
scale_x_continuous(labels = percent_format(),         # Format x-axis as percentages
breaks = seq(0, 1, 0.1)) +         # Show 10% increments
scale_y_continuous(labels = percent_format()) +       # Format y-axis as percentages
labs(x = "Approval Rate",
y = "Bad Rate",
title = "Bad Rate by Approval Rate",
subtitle = "Comparison of Population vs. Segment Model") +
theme_minimal() +                                     # Clean visual theme
theme(legend.title = element_blank(),                 # Remove legend title
legend.position = "bottom")                      # Position legend at bottom
combined
combined
ggplot(combined, aes(x = cum_pct, y = cum_bad_rate, color = model, group = model)) +
geom_line(size = 1) +                                  # Connect points with lines
geom_point(size = 3)
pop_perf
smple %>%
filter(segment == "KG")
#| label: model-comparison
# Evaluate population model performance on the KG segment
pop_perf <- smple %>%
filter(segment == "KG") %>%                     # Focus on KG segment only
mutate(pred = predict(mdl_pop, newdata = .),   # Get model predictions
score = scaling_func(pred),             # Convert to credit score scale
total = n()) %>%                         # Total number of KG customers
arrange(desc(score)) %>%                        # Sort by score (best first)
mutate(cum_count = row_number(),               # Cumulative count
cum_pct = cum_count / total,            # Approval rate
cum_bad = cumsum(bad_flag),             # Cumulative bad loans
cum_bad_rate = cum_bad / cum_count) %>% # Bad rate at each approval threshold
filter(cum_pct %in% seq(0.1, 1, 0.1))          # Sample at 10% intervals
# Evaluate segment model performance on the KG segment
seg_perf <- smple %>%
filter(segment == "KG") %>%                     # Same segment
mutate(pred = predict(mdl_seg, newdata = .),   # But using segment model
score = scaling_func(pred),
total = n()) %>%
arrange(desc(score)) %>%
mutate(cum_count = row_number(),
cum_pct = cum_count / total,
cum_bad = cumsum(bad_flag),
cum_bad_rate = cum_bad / cum_count) %>%
filter(cum_pct %in% seq(0.1, 1, 0.1))
#| label: visualization
# Combine results from both models for plotting
pop_perf$model <- "Population Model"
seg_perf$model <- "Segment Model"
combined <- rbind(pop_perf, seg_perf)
# Create a line plot comparing bad rates at different approval thresholds
library(ggplot2)
ggplot(combined, aes(x = cum_pct, y = cum_bad_rate, color = model, group = model)) +
geom_line(size = 1) +                                  # Connect points with lines
geom_point(size = 3) +                                 # Add points at each threshold
scale_x_continuous(labels = percent_format(),         # Format x-axis as percentages
breaks = seq(0, 1, 0.1)) +         # Show 10% increments
scale_y_continuous(labels = percent_format()) +       # Format y-axis as percentages
labs(x = "Approval Rate",
y = "Bad Rate",
title = "Bad Rate by Approval Rate",
subtitle = "Comparison of Population vs. Segment Model") +
theme_minimal() +                                     # Clean visual theme
theme(legend.title = element_blank(),                 # Remove legend title
legend.position = "bottom")                      # Position legend at bottom
smple %>%
filter(segment == "KG") %>%                     # Focus on KG segment only
mutate(pred = predict(mdl_pop, newdata = .),   # Get model predictions
score = scaling_func(pred),             # Convert to credit score scale
total = n()) %>%                         # Total number of KG customers
arrange(desc(score)) %>%                        # Sort by score (best first)
mutate(cum_count = row_number(),               # Cumulative count
cum_pct = cum_count / total,            # Approval rate
cum_bad = cumsum(bad_flag),             # Cumulative bad loans
cum_bad_rate = cum_bad / cum_count)
smple %>%
filter(segment == "KG") %>%                     # Focus on KG segment only
mutate(pred = predict(mdl_pop, newdata = .),   # Get model predictions
score = scaling_func(pred),             # Convert to credit score scale
total = n()) %>%                         # Total number of KG customers
arrange(desc(score)) %>%                        # Sort by score (best first)
mutate(cum_count = row_number(),               # Cumulative count
cum_pct = cum_count / total,            # Approval rate
cum_bad = cumsum(bad_flag),             # Cumulative bad loans
cum_bad_rate = cum_bad / cum_count)
smple %>%
filter(segment == "KG") %>%                     # Focus on KG segment only
mutate(pred = predict(mdl_pop, newdata = .),   # Get model predictions
score = scaling_func(pred),             # Convert to credit score scale
total = n()) %>%                         # Total number of KG customers
arrange(desc(score)) %>%                        # Sort by score (best first)
mutate(cum_count = row_number(),               # Cumulative count
cum_pct = cum_count / total,            # Approval rate
cum_bad = cumsum(bad_flag),             # Cumulative bad loans
cum_bad_rate = cum_bad / cum_count) %>% # Bad rate at each approval threshold
filter(cum_pct %in% seq(0.1, 1, 0.1))
smple %>%
filter(segment == "KG") %>%                     # Focus on KG segment only
mutate(pred = predict(mdl_pop, newdata = .),   # Get model predictions
score = scaling_func(pred),             # Convert to credit score scale
total = n()) %>%                         # Total number of KG customers
arrange(desc(score)) %>%                        # Sort by score (best first)
mutate(cum_count = row_number(),               # Cumulative count
cum_pct = cum_count / total,            # Approval rate
cum_bad = cumsum(bad_flag),             # Cumulative bad loans
cum_bad_rate = cum_bad / cum_count) %>% # Bad rate at each approval threshold
filter(cum_pct %in% seq(0.1, 1, 0.1)) %>% View
smple %>%
filter(segment == "KG") %>%                     # Focus on KG segment only
mutate(pred = predict(mdl_pop, newdata = .),   # Get model predictions
score = scaling_func(pred),             # Convert to credit score scale
total = n()) %>%                         # Total number of KG customers
arrange(desc(score)) %>%                        # Sort by score (best first)
mutate(cum_count = row_number(),               # Cumulative count
cum_pct = cum_count / total,            # Approval rate
cum_bad = cumsum(bad_flag),             # Cumulative bad loans
cum_bad_rate = cum_bad / cum_count) %>% View
smple %>%
filter(segment == "KG") %>%                     # Focus on KG segment only
mutate(pred = predict(mdl_pop, newdata = .),   # Get model predictions
score = scaling_func(pred),             # Convert to credit score scale
total = n()) %>%                         # Total number of KG customers
arrange(desc(score)) %>%                        # Sort by score (best first)
mutate(cum_count = row_number(),               # Cumulative count
cum_pct = cum_count / total,            # Approval rate
cum_bad = cumsum(bad_flag),             # Cumulative bad loans
cum_bad_rate = cum_bad / cum_count)
smple %>%
filter(segment == "KG") %>%                     # Focus on KG segment only
mutate(pred = predict(mdl_pop, newdata = .),   # Get model predictions
score = scaling_func(pred),             # Convert to credit score scale
total = n()) %>%                         # Total number of KG customers
arrange(desc(score)) %>%                        # Sort by score (best first)
mutate(cum_count = row_number(),               # Cumulative count
cum_pct = cum_count / total,            # Approval rate
cum_bad = cumsum(bad_flag),             # Cumulative bad loans
cum_bad_rate = cum_bad / cum_count) %>% View
