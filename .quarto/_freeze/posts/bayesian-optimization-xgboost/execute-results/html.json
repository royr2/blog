{
  "hash": "9f19f1ebc77a3857a650058dcf4e3d86",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Using Bayesian Optimization to Tune XGBoost Models in R\"\ndate: \"2024-09-18\"\ncategories: [R, Analytics, Machine Learning]\nimage: \"../images/bayesian_opt.png\"\nexecute:\n  echo: true\n  warning: false\n  message: false\n  eval: true\n  cache: true\n---\n\n\n\nTuning machine learning models can be time-consuming and computationally expensive. This post shows how to use Bayesian optimization to efficiently find optimal XGBoost hyperparameters – saving time and improving model performance.\n\n## Required Packages\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load required packages\nlibrary(xgboost)\nlibrary(ParBayesianOptimization)\nlibrary(mlbench)\nlibrary(dplyr)\nlibrary(recipes)\nlibrary(rsample)\n```\n:::\n\n\n\n## Data Preparation\n\nWe'll use the Boston Housing dataset – a classic regression problem with both numeric and categorical variables.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the Boston Housing dataset\ndata(\"BostonHousing2\")\n\n# Quick look at the data structure\nstr(BostonHousing2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t506 obs. of  19 variables:\n $ town   : Factor w/ 92 levels \"Arlington\",\"Ashland\",..: 54 77 77 46 46 46 69 69 69 69 ...\n $ tract  : int  2011 2021 2022 2031 2032 2033 2041 2042 2043 2044 ...\n $ lon    : num  -71 -71 -70.9 -70.9 -70.9 ...\n $ lat    : num  42.3 42.3 42.3 42.3 42.3 ...\n $ medv   : num  24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ...\n $ cmedv  : num  24 21.6 34.7 33.4 36.2 28.7 22.9 22.1 16.5 18.9 ...\n $ crim   : num  0.00632 0.02731 0.02729 0.03237 0.06905 ...\n $ zn     : num  18 0 0 0 0 0 12.5 12.5 12.5 12.5 ...\n $ indus  : num  2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ...\n $ chas   : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ nox    : num  0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ...\n $ rm     : num  6.58 6.42 7.18 7 7.15 ...\n $ age    : num  65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ...\n $ dis    : num  4.09 4.97 4.97 6.06 6.06 ...\n $ rad    : int  1 2 2 3 3 3 5 5 5 5 ...\n $ tax    : int  296 242 242 222 222 222 311 311 311 311 ...\n $ ptratio: num  15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ...\n $ b      : num  397 397 393 395 397 ...\n $ lstat  : num  4.98 9.14 4.03 2.94 5.33 ...\n```\n\n\n:::\n:::\n\n\n\nXGBoost requires numeric inputs, so we'll use the `recipes` package to transform our categorical variables:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a recipe for preprocessing\nrec <- recipe(cmedv ~ ., data = BostonHousing2) %>%\n  # Collapse categories where population is < 3%\n  step_other(town, chas, threshold = .03, other = \"Other\") %>% \n  # Create dummy variables for all factor variables \n  step_dummy(all_nominal_predictors())\n\n# Train the recipe on the dataset\nprep <- prep(rec, training = BostonHousing2)\n\n# Create the final model matrix\nmodel_df <- bake(prep, new_data = BostonHousing2)\n\n# Check the column names after one-hot encoding\ncolnames(model_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"tract\"                  \"lon\"                    \"lat\"                   \n [4] \"medv\"                   \"crim\"                   \"zn\"                    \n [7] \"indus\"                  \"nox\"                    \"rm\"                    \n[10] \"age\"                    \"dis\"                    \"rad\"                   \n[13] \"tax\"                    \"ptratio\"                \"b\"                     \n[16] \"lstat\"                  \"cmedv\"                  \"town_Boston.Savin.Hill\"\n[19] \"town_Cambridge\"         \"town_Lynn\"              \"town_Newton\"           \n[22] \"town_Other\"             \"chas_X1\"               \n```\n\n\n:::\n:::\n\n\n\nNext, we'll split our data into training and testing sets:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a 70/30 train-test split\nsplits <- rsample::initial_split(model_df, prop = 0.7)\ntrain_df <- rsample::training(splits)\ntest_df <- rsample::testing(splits)\n\n# Prepare the training data for XGBoost\nX <- train_df %>%\n  select(!medv, !cmedv) %>%\n  as.matrix()\n\n# Get the target variable\ny <- train_df %>% pull(cmedv)\n\n# Create cross-validation folds\nfolds <- list(\n  fold1 = as.integer(seq(1, nrow(X), by = 5)),\n  fold2 = as.integer(seq(2, nrow(X), by = 5))\n)\n```\n:::\n\n\n\n## Setting Up Bayesian Optimization\n\nBayesian optimization requires two key components:\n\n1. An objective function that evaluates model performance\n2. The parameter bounds we want to explore\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Our objective function takes hyperparameters as inputs\nobj_func <- function(eta, max_depth, min_child_weight, subsample, lambda, alpha) {\n  \n  param <- list(\n    # Learning parameters\n    eta = eta,                       # Learning rate\n    max_depth = max_depth,           # Tree depth\n    min_child_weight = min_child_weight, # Min observations per node\n    subsample = subsample,           # Data subsampling\n    lambda = lambda,                 # L2 regularization\n    alpha = alpha,                   # L1 regularization\n    \n    booster = \"gbtree\",             # Use tree model\n    objective = \"reg:squarederror\",  # Regression task\n    eval_metric = \"mape\"            # Mean Absolute Percentage Error\n  )\n  \n  xgbcv <- xgb.cv(params = param,\n                  data = X,\n                  label = y,\n                  nround = 50,\n                  folds = folds,\n                  prediction = TRUE,\n                  early_stopping_rounds = 5,\n                  verbose = 0,\n                  maximize = FALSE)\n  \n  lst <- list(\n    # First argument must be named as \"Score\"\n    # Function finds maxima so inverting the output\n    Score = -min(xgbcv$evaluation_log$test_mape_mean),\n    \n    # Get number of trees for the best performing model\n    nrounds = xgbcv$best_iteration\n  )\n  \n  return(lst)\n}\n\n# Define the search space for each parameter\nbounds <- list(\n  eta = c(0.001, 0.2),             # Learning rate range\n  max_depth = c(1L, 10L),           # Tree depth range\n  min_child_weight = c(1, 50),      # Min observations range\n  subsample = c(0.1, 1),            # Subsampling range\n  lambda = c(1, 10),                # L2 regularization range\n  alpha = c(1, 10)                  # L1 regularization range\n)\n```\n:::\n\n\n\n## Running Bayesian Optimization\n\nNow we'll run the optimization process to intelligently search for the best parameters:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nbayes_out <- bayesOpt(\n  FUN = obj_func,                    # Our objective function\n  bounds = bounds,                   # Parameter bounds\n  initPoints = length(bounds) + 2,   # Initial random points\n  iters.n = 10,                      # Number of iterations\n  verbose = 0                        # Suppress output\n)\n\n# View top results\nbayes_out$scoreSummary[1:5, c(3:8, 13)]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          eta max_depth min_child_weight subsample   lambda    alpha      Score\n        <num>     <num>            <num>     <num>    <num>    <num>      <num>\n1: 0.13392137         8         4.913332 0.2105925 4.721124 3.887629 -0.1292920\n2: 0.19400811         2        25.454160 0.9594105 9.329695 3.173695 -0.1790158\n3: 0.16079775         2        14.035652 0.5118349 1.229953 5.093530 -0.1662595\n4: 0.08957707         4        12.534842 0.3844404 4.358837 1.788342 -0.1672395\n5: 0.02876388         4        36.586761 0.8107181 6.137100 6.039125 -0.3320015\n```\n\n\n:::\n\n```{.r .cell-code}\n# Get the best parameters\nbest_params <- getBestPars(bayes_out)\ndata.frame(best_params)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        eta max_depth min_child_weight subsample lambda    alpha\n1 0.1251447        10                1         1      1 5.905011\n```\n\n\n:::\n:::\n\n\n\n## Training the Final Model\n\nWith the optimal hyperparameters identified, we can now train our final XGBoost model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Combine best params with base params\nopt_params <- append(\n  list(booster = \"gbtree\", \n       objective = \"reg:squarederror\", \n       eval_metric = \"mae\"), \n  best_params\n)\n\n# Run cross-validation to determine optimal number of rounds\nxgbcv <- xgb.cv(\n  params = opt_params,\n  data = X,\n  label = y,\n  nround = 100,\n  folds = folds,\n  prediction = TRUE,\n  early_stopping_rounds = 5,\n  verbose = 0,\n  maximize = FALSE\n)\n\n# Get optimal number of rounds\nnrounds = xgbcv$best_iteration\n\n# Fit the final XGBoost model\nmdl <- xgboost(\n  data = X, \n  label = y, \n  params = opt_params, \n  maximize = FALSE, \n  early_stopping_rounds = 5, \n  nrounds = nrounds, \n  verbose = 0\n)\n\n# Make predictions on the test set\nactuals <- test_df$cmedv\npredicted <- test_df %>%\n  select_at(mdl$feature_names) %>%\n  as.matrix() %>%\n  predict(mdl, newdata = .)\n\n# Evaluate performance using Mean Absolute Percentage Error (MAPE)\nmape <- mean(abs(actuals - predicted)/actuals)\ncat(\"MAPE on test set:\", mape)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMAPE on test set: 0.006424492\n```\n\n\n:::\n:::\n\n\n## Why Bayesian Optimization \n\nBayesian optimization offers several key advantages over traditional grid search:\n\n1. **Efficiency**: Finds optimal parameters in fewer iterations\n2. **Intelligence**: Learns from previous evaluations to focus on promising areas\n3. **Scalability**: Remains efficient even with many hyperparameters\n4. **Speed**: Completes in a fraction of the time while achieving comparable or better results\n\nThis approach becomes increasingly valuable as model complexity grows. For production models, consider increasing the iterations (`iters.n`) to ensure thorough exploration of the parameter space.\n\nThe `ParBayesianOptimization` package makes this powerful technique accessible to R users, allowing you to build better models with less computational overhead.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}