{"title":"Using Bayesian Optimization to Tune XGBoost Models in R","markdown":{"yaml":{"title":"Using Bayesian Optimization to Tune XGBoost Models in R","date":"2024-09-18","categories":["R","Analytics","Machine Learning"],"image":"../images/bayesian_opt.png","execute":{"echo":true,"warning":false,"message":false,"eval":true,"cache":true}},"headingText":"Required Packages","containsRefs":false,"markdown":"\n\nTuning machine learning models can be time-consuming and computationally expensive. This post shows how to use Bayesian optimization to efficiently find optimal XGBoost hyperparameters – saving time and improving model performance.\n\n\n```{r}\n#| label: setup\n#| message: false\n#| warning: false\n\n# Load required packages\nlibrary(xgboost)\nlibrary(ParBayesianOptimization)\nlibrary(mlbench)\nlibrary(dplyr)\nlibrary(recipes)\nlibrary(rsample)\n```\n\n## Data Preparation\n\nWe'll use the Boston Housing dataset – a classic regression problem with both numeric and categorical variables.\n\n```{r}\n#| label: data-load\n\n# Load the Boston Housing dataset\ndata(\"BostonHousing2\")\n\n# Quick look at the data structure\nstr(BostonHousing2)\n```\n\nXGBoost requires numeric inputs, so we'll use the `recipes` package to transform our categorical variables:\n\n```{r}\n#| label: data-prep\n\n# Create a recipe for preprocessing\nrec <- recipe(cmedv ~ ., data = BostonHousing2) %>%\n  # Collapse categories where population is < 3%\n  step_other(town, chas, threshold = .03, other = \"Other\") %>% \n  # Create dummy variables for all factor variables \n  step_dummy(all_nominal_predictors())\n\n# Train the recipe on the dataset\nprep <- prep(rec, training = BostonHousing2)\n\n# Create the final model matrix\nmodel_df <- bake(prep, new_data = BostonHousing2)\n\n# Check the column names after one-hot encoding\ncolnames(model_df)\n```\n\nNext, we'll split our data into training and testing sets:\n\n```{r}\n#| label: train-test-split\n\n# Create a 70/30 train-test split\nsplits <- rsample::initial_split(model_df, prop = 0.7)\ntrain_df <- rsample::training(splits)\ntest_df <- rsample::testing(splits)\n\n# Prepare the training data for XGBoost\nX <- train_df %>%\n  select(!medv, !cmedv) %>%\n  as.matrix()\n\n# Get the target variable\ny <- train_df %>% pull(cmedv)\n\n# Create cross-validation folds\nfolds <- list(\n  fold1 = as.integer(seq(1, nrow(X), by = 5)),\n  fold2 = as.integer(seq(2, nrow(X), by = 5))\n)\n```\n\n## Setting Up Bayesian Optimization\n\nBayesian optimization requires two key components:\n\n1. An objective function that evaluates model performance\n2. The parameter bounds we want to explore\n\n```{r}\n#| label: objective-function\n\n# Our objective function takes hyperparameters as inputs\nobj_func <- function(eta, max_depth, min_child_weight, subsample, lambda, alpha) {\n  \n  param <- list(\n    # Learning parameters\n    eta = eta,                       # Learning rate\n    max_depth = max_depth,           # Tree depth\n    min_child_weight = min_child_weight, # Min observations per node\n    subsample = subsample,           # Data subsampling\n    lambda = lambda,                 # L2 regularization\n    alpha = alpha,                   # L1 regularization\n    \n    booster = \"gbtree\",             # Use tree model\n    objective = \"reg:squarederror\",  # Regression task\n    eval_metric = \"mape\"            # Mean Absolute Percentage Error\n  )\n  \n  xgbcv <- xgb.cv(params = param,\n                  data = X,\n                  label = y,\n                  nround = 50,\n                  folds = folds,\n                  prediction = TRUE,\n                  early_stopping_rounds = 5,\n                  verbose = 0,\n                  maximize = FALSE)\n  \n  lst <- list(\n    # First argument must be named as \"Score\"\n    # Function finds maxima so inverting the output\n    Score = -min(xgbcv$evaluation_log$test_mape_mean),\n    \n    # Get number of trees for the best performing model\n    nrounds = xgbcv$best_iteration\n  )\n  \n  return(lst)\n}\n\n# Define the search space for each parameter\nbounds <- list(\n  eta = c(0.001, 0.2),             # Learning rate range\n  max_depth = c(1L, 10L),           # Tree depth range\n  min_child_weight = c(1, 50),      # Min observations range\n  subsample = c(0.1, 1),            # Subsampling range\n  lambda = c(1, 10),                # L2 regularization range\n  alpha = c(1, 10)                  # L1 regularization range\n)\n```\n\n## Running Bayesian Optimization\n\nNow we'll run the optimization process to intelligently search for the best parameters:\n\n```{r}\n#| label: bayesian-opt\n\nset.seed(1234)\nbayes_out <- bayesOpt(\n  FUN = obj_func,                    # Our objective function\n  bounds = bounds,                   # Parameter bounds\n  initPoints = length(bounds) + 2,   # Initial random points\n  iters.n = 10,                      # Number of iterations\n  verbose = 0                        # Suppress output\n)\n\n# View top results\nbayes_out$scoreSummary[1:5, c(3:8, 13)]\n\n# Get the best parameters\nbest_params <- getBestPars(bayes_out)\ndata.frame(best_params)\n```\n\n## Training the Final Model\n\nWith the optimal hyperparameters identified, we can now train our final XGBoost model.\n\n```{r}\n#| label: final-model\n\n# Combine best params with base params\nopt_params <- append(\n  list(booster = \"gbtree\", \n       objective = \"reg:squarederror\", \n       eval_metric = \"mae\"), \n  best_params\n)\n\n# Run cross-validation to determine optimal number of rounds\nxgbcv <- xgb.cv(\n  params = opt_params,\n  data = X,\n  label = y,\n  nround = 100,\n  folds = folds,\n  prediction = TRUE,\n  early_stopping_rounds = 5,\n  verbose = 0,\n  maximize = FALSE\n)\n\n# Get optimal number of rounds\nnrounds = xgbcv$best_iteration\n\n# Fit the final XGBoost model\nmdl <- xgboost(\n  data = X, \n  label = y, \n  params = opt_params, \n  maximize = FALSE, \n  early_stopping_rounds = 5, \n  nrounds = nrounds, \n  verbose = 0\n)\n\n# Make predictions on the test set\nactuals <- test_df$cmedv\npredicted <- test_df %>%\n  select_at(mdl$feature_names) %>%\n  as.matrix() %>%\n  predict(mdl, newdata = .)\n\n# Evaluate performance using Mean Absolute Percentage Error (MAPE)\nmape <- mean(abs(actuals - predicted)/actuals)\ncat(\"MAPE on test set:\", mape)\n```\n## Why Bayesian Optimization \n\nBayesian optimization offers several key advantages over traditional grid search:\n\n1. **Efficiency**: Finds optimal parameters in fewer iterations\n2. **Intelligence**: Learns from previous evaluations to focus on promising areas\n3. **Scalability**: Remains efficient even with many hyperparameters\n4. **Speed**: Completes in a fraction of the time while achieving comparable or better results\n\nThis approach becomes increasingly valuable as model complexity grows. For production models, consider increasing the iterations (`iters.n`) to ensure thorough exploration of the parameter space.\n\nThe `ParBayesianOptimization` package makes this powerful technique accessible to R users, allowing you to build better models with less computational overhead.\n","srcMarkdownNoYaml":"\n\nTuning machine learning models can be time-consuming and computationally expensive. This post shows how to use Bayesian optimization to efficiently find optimal XGBoost hyperparameters – saving time and improving model performance.\n\n## Required Packages\n\n```{r}\n#| label: setup\n#| message: false\n#| warning: false\n\n# Load required packages\nlibrary(xgboost)\nlibrary(ParBayesianOptimization)\nlibrary(mlbench)\nlibrary(dplyr)\nlibrary(recipes)\nlibrary(rsample)\n```\n\n## Data Preparation\n\nWe'll use the Boston Housing dataset – a classic regression problem with both numeric and categorical variables.\n\n```{r}\n#| label: data-load\n\n# Load the Boston Housing dataset\ndata(\"BostonHousing2\")\n\n# Quick look at the data structure\nstr(BostonHousing2)\n```\n\nXGBoost requires numeric inputs, so we'll use the `recipes` package to transform our categorical variables:\n\n```{r}\n#| label: data-prep\n\n# Create a recipe for preprocessing\nrec <- recipe(cmedv ~ ., data = BostonHousing2) %>%\n  # Collapse categories where population is < 3%\n  step_other(town, chas, threshold = .03, other = \"Other\") %>% \n  # Create dummy variables for all factor variables \n  step_dummy(all_nominal_predictors())\n\n# Train the recipe on the dataset\nprep <- prep(rec, training = BostonHousing2)\n\n# Create the final model matrix\nmodel_df <- bake(prep, new_data = BostonHousing2)\n\n# Check the column names after one-hot encoding\ncolnames(model_df)\n```\n\nNext, we'll split our data into training and testing sets:\n\n```{r}\n#| label: train-test-split\n\n# Create a 70/30 train-test split\nsplits <- rsample::initial_split(model_df, prop = 0.7)\ntrain_df <- rsample::training(splits)\ntest_df <- rsample::testing(splits)\n\n# Prepare the training data for XGBoost\nX <- train_df %>%\n  select(!medv, !cmedv) %>%\n  as.matrix()\n\n# Get the target variable\ny <- train_df %>% pull(cmedv)\n\n# Create cross-validation folds\nfolds <- list(\n  fold1 = as.integer(seq(1, nrow(X), by = 5)),\n  fold2 = as.integer(seq(2, nrow(X), by = 5))\n)\n```\n\n## Setting Up Bayesian Optimization\n\nBayesian optimization requires two key components:\n\n1. An objective function that evaluates model performance\n2. The parameter bounds we want to explore\n\n```{r}\n#| label: objective-function\n\n# Our objective function takes hyperparameters as inputs\nobj_func <- function(eta, max_depth, min_child_weight, subsample, lambda, alpha) {\n  \n  param <- list(\n    # Learning parameters\n    eta = eta,                       # Learning rate\n    max_depth = max_depth,           # Tree depth\n    min_child_weight = min_child_weight, # Min observations per node\n    subsample = subsample,           # Data subsampling\n    lambda = lambda,                 # L2 regularization\n    alpha = alpha,                   # L1 regularization\n    \n    booster = \"gbtree\",             # Use tree model\n    objective = \"reg:squarederror\",  # Regression task\n    eval_metric = \"mape\"            # Mean Absolute Percentage Error\n  )\n  \n  xgbcv <- xgb.cv(params = param,\n                  data = X,\n                  label = y,\n                  nround = 50,\n                  folds = folds,\n                  prediction = TRUE,\n                  early_stopping_rounds = 5,\n                  verbose = 0,\n                  maximize = FALSE)\n  \n  lst <- list(\n    # First argument must be named as \"Score\"\n    # Function finds maxima so inverting the output\n    Score = -min(xgbcv$evaluation_log$test_mape_mean),\n    \n    # Get number of trees for the best performing model\n    nrounds = xgbcv$best_iteration\n  )\n  \n  return(lst)\n}\n\n# Define the search space for each parameter\nbounds <- list(\n  eta = c(0.001, 0.2),             # Learning rate range\n  max_depth = c(1L, 10L),           # Tree depth range\n  min_child_weight = c(1, 50),      # Min observations range\n  subsample = c(0.1, 1),            # Subsampling range\n  lambda = c(1, 10),                # L2 regularization range\n  alpha = c(1, 10)                  # L1 regularization range\n)\n```\n\n## Running Bayesian Optimization\n\nNow we'll run the optimization process to intelligently search for the best parameters:\n\n```{r}\n#| label: bayesian-opt\n\nset.seed(1234)\nbayes_out <- bayesOpt(\n  FUN = obj_func,                    # Our objective function\n  bounds = bounds,                   # Parameter bounds\n  initPoints = length(bounds) + 2,   # Initial random points\n  iters.n = 10,                      # Number of iterations\n  verbose = 0                        # Suppress output\n)\n\n# View top results\nbayes_out$scoreSummary[1:5, c(3:8, 13)]\n\n# Get the best parameters\nbest_params <- getBestPars(bayes_out)\ndata.frame(best_params)\n```\n\n## Training the Final Model\n\nWith the optimal hyperparameters identified, we can now train our final XGBoost model.\n\n```{r}\n#| label: final-model\n\n# Combine best params with base params\nopt_params <- append(\n  list(booster = \"gbtree\", \n       objective = \"reg:squarederror\", \n       eval_metric = \"mae\"), \n  best_params\n)\n\n# Run cross-validation to determine optimal number of rounds\nxgbcv <- xgb.cv(\n  params = opt_params,\n  data = X,\n  label = y,\n  nround = 100,\n  folds = folds,\n  prediction = TRUE,\n  early_stopping_rounds = 5,\n  verbose = 0,\n  maximize = FALSE\n)\n\n# Get optimal number of rounds\nnrounds = xgbcv$best_iteration\n\n# Fit the final XGBoost model\nmdl <- xgboost(\n  data = X, \n  label = y, \n  params = opt_params, \n  maximize = FALSE, \n  early_stopping_rounds = 5, \n  nrounds = nrounds, \n  verbose = 0\n)\n\n# Make predictions on the test set\nactuals <- test_df$cmedv\npredicted <- test_df %>%\n  select_at(mdl$feature_names) %>%\n  as.matrix() %>%\n  predict(mdl, newdata = .)\n\n# Evaluate performance using Mean Absolute Percentage Error (MAPE)\nmape <- mean(abs(actuals - predicted)/actuals)\ncat(\"MAPE on test set:\", mape)\n```\n## Why Bayesian Optimization \n\nBayesian optimization offers several key advantages over traditional grid search:\n\n1. **Efficiency**: Finds optimal parameters in fewer iterations\n2. **Intelligence**: Learns from previous evaluations to focus on promising areas\n3. **Scalability**: Remains efficient even with many hyperparameters\n4. **Speed**: Completes in a fraction of the time while achieving comparable or better results\n\nThis approach becomes increasingly valuable as model complexity grows. For production models, consider increasing the iterations (`iters.n`) to ensure thorough exploration of the parameter space.\n\nThe `ParBayesianOptimization` package makes this powerful technique accessible to R users, allowing you to build better models with less computational overhead.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"message":false,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"zenburn","toc":true,"css":["../styles.css"],"output-file":"bayesian-optimization-xgboost.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.42","editor":"source","theme":"sketchy","code-copy":true,"title":"Using Bayesian Optimization to Tune XGBoost Models in R","date":"2024-09-18","categories":["R","Analytics","Machine Learning"],"image":"../images/bayesian_opt.png"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}