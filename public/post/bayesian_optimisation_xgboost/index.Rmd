---
title: "Using bayesian optimisation to tune a XGBOOST model in R"
subtitle: "ML series (Post #1)"
summary: "How to use bayesian optimisation to tune hyperparameters in a XGBOOST model in R"
author: "royr2"
date: 2022-01-08
categories: ["R", "risk analytics", "xgboost", "bayesian optimisation", "machine learning"]
tags: ["R", "analytics", "machine learning"]  
comments: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

My first post in 2022! A very happy new year to anyone reading this. :smile:

I was looking for a simple and effective way to tune `xgboost` models in `R` and came across this package called [ParBayesianOptimization](https://github.com/AnotherSamWilson/ParBayesianOptimization). Here's a quick tutorial on how to use it to tune a `xgboost` model. 

```{r eval = F}
# Pacman is a package management tool 
install.packages("pacman")
```

```{r message = F, warning = F}
library(pacman)

# p_load automatically installs packages if needed
p_load(xgboost, ParBayesianOptimization, mlbench, dplyr, skimr, recipes, resample)
```

## Data prep

```{r}
# Load up some data
data("BostonHousing2")
```

```{r results='asis'}
# Data summary
skim(BostonHousing2)
```

Looks like there is are two factor variables. We'll need to convert them into numeric variables before we proceed. I'll use the `recipes` package to one-hot encode them. 

```{r}
# Predicting median house prices
rec <- recipe(cmedv ~ ., data = BostonHousing2) %>%
  
  # Collapse categories where population is < 3%
  step_other(town, chas, threshold = .03, other = "Other") %>% 
  
  # Create dummy variables for all factor variables 
  step_dummy(all_nominal_predictors())

# Train the recipe on the data set
prep <- prep(rec, training = BostonHousing2)

# Create the final model matrix
model_df <- bake(prep, new_data = BostonHousing2)
```

```{r}
# All levels have been one hot encoded and separate columns have been appended to the model matrix
colnames(model_df)
```
Next, we can use the `resample` package to create test/train splits.

```{r}
splits <- rsample::initial_split(model_df, prop = 0.7)

# Training set
train_df <- rsample::training(splits)

# Test set
test_df <- rsample::testing(splits)
```

```{r}
dim(train_df)
```

```{r}
dim(test_df)
```
## Finding optimal parameters

Now we can start to run some optimisations using the `ParBayesianOptimization` package.

```{r}
# The xgboost interface accepts matrices 
X <- train_df %>%
  # Remove the target variable
  select(!medv, !cmedv) %>%
  as.matrix()

# Get the target variable
y <- train_df %>%
  pull(cmedv)
```

```{r}
# Cross validation folds
folds <- list(fold1 = as.integer(seq(1, nrow(X), by = 5)),
              fold2 = as.integer(seq(2, nrow(X), by = 5)))
```

We'll need an objective function which can be fed to the optimiser. We'll use the value of the evaluation metric from `xgb.cv()` as the value that needs to be optimised. 

```{r}
# Function must take the hyper-parameters as inputs
obj_func <- function(eta, max_depth, min_child_weight, subsample, lambda, alpha) {
  
  param <- list(
    
    # Hyter parameters 
    eta = eta,
    max_depth = max_depth,
    min_child_weight = min_child_weight,
    subsample = subsample,
    lambda = lambda,
    alpha = alpha,
    
    # Tree model 
    booster = "gbtree",
    
    # Regression problem 
    objective = "reg:squarederror",
    
    # Use the Mean Absolute Percentage Error
    eval_metric = "mape")
  
  xgbcv <- xgb.cv(params = param,
                  data = X,
                  label = y,
                  nround = 50,
                  folds = folds,
                  prediction = TRUE,
                  early_stopping_rounds = 5,
                  verbose = 0,
                  maximize = F)
  
  lst <- list(
    
    # First argument must be named as "Score"
    # Function finds maxima so inverting the output
    Score = -min(xgbcv$evaluation_log$test_mape_mean),
    
    # Get number of trees for the best performing model
    nrounds = xgbcv$best_iteration
  )
  
  return(lst)
}
```

Once we have the objective function, we'll need to define some bounds for the optimiser to search within.  

```{r}
bounds <- list(eta = c(0.001, 0.2),
               max_depth = c(1L, 10L),
               min_child_weight = c(1, 50),
               subsample = c(0.1, 1),
               lambda = c(1, 10),
               alpha = c(1, 10))
```

We can now run the optimiser to find a set of optimal hyper-parameters. 

```{r echo = F, eval = F}
set.seed(1234)
optObj <- bayesOpt(FUN = obj_func, bounds = bounds, initPoints = length(bounds) + 2, iters.n = 3)

saveRDS(optObj, "opt.RDS")
```

```{r eval = F}
set.seed(1234)
bayes_out <- bayesOpt(FUN = obj_func, bounds = bounds, initPoints = length(bounds) + 2, iters.n = 3)
```

```{r echo = F}
bayes_out <- readRDS("opt.RDS")
```

```{r}
# Show relevant columns from the summary object 
bayes_out$scoreSummary[1:5, c(3:8, 13)]
```

```{r}
# Get best parameters
data.frame(getBestPars(bayes_out))
```

## Fitting the model

We can now fit a model and check how well these parameters work.

```{r}
# Combine best params with base params
opt_params <- append(list(booster = "gbtree", 
                          objective = "reg:squarederror", 
                          eval_metric = "mae"), 
                     getBestPars(bayes_out))

# Run cross validation 
xgbcv <- xgb.cv(params = opt_params,
                data = X,
                label = y,
                nround = 100,
                folds = folds,
                prediction = TRUE,
                early_stopping_rounds = 5,
                verbose = 0,
                maximize = F)

# Get optimal number of rounds
nrounds = xgbcv$best_iteration

# Fit a xgb model
mdl <- xgboost(data = X, label = y, 
               params = opt_params, 
               maximize = F, 
               early_stopping_rounds = 5, 
               nrounds = nrounds, 
               verbose = 0)
```

```{r}
# Evaluate performance 
actuals <- test_df$cmedv
predicted <- test_df %>%
  select_at(mdl$feature_names) %>%
  as.matrix %>%
  predict(mdl, newdata = .)
```

```{r}
# Compute MAPE
mean(abs(actuals - predicted)/actuals)
```
## Compare with grid search 

```{r eval = F}
grd <- expand.grid(
  eta = seq(0.001, 0.2, length.out = 5),
  max_depth = seq(2L, 10L, by = 1),
  min_child_weight = seq(1, 25, length.out = 3),
  subsample = c(0.25, 0.5, 0.75, 1),
  lambda = c(1, 5, 10),
  alpha = c(1, 5, 10))

dim(grd)
```

```{r eval = F}
grd_out <- apply(grd, 1, function(par){
  
    par <- append(par, list(booster = "gbtree",objective = "reg:squarederror",eval_metric = "mae"))
    mdl <- xgboost(data = X, label = y, params = par, nrounds = 50, early_stopping_rounds = 5, maximize = F, verbose = 0)
    lst <- data.frame(par, score = mdl$best_score)

    return(lst)
  })

grd_out <- do.call(rbind, grd_out)
```

```{r echo = F, eval = F}
saveRDS(grd_out, "grid_out.RDS")
```

```{r echo = F}
grd_out <- readRDS("grid_out.RDS")
```

```{r}
best_par <- grd_out %>%
  data.frame() %>%
  arrange(score) %>%
  .[1,]
```

```{r}
# Fit final model
params <- as.list(best_par[-length(best_par)])
xgbcv <- xgb.cv(params = params,
                data = X,
                label = y,
                nround = 100,
                folds = folds,
                prediction = TRUE,
                early_stopping_rounds = 5,
                verbose = 0,
                maximize = F)

nrounds = xgbcv$best_iteration

mdl <- xgboost(data = X, 
               label = y, 
               params = params, 
               maximize = F, 
               early_stopping_rounds = 5, 
               nrounds = nrounds, 
               verbose = 0)
```

```{r}
# Evaluate on test set
act <- test_df$medv
pred <- test_df %>%
  select_at(mdl$feature_names) %>%
  as.matrix %>%
  predict(mdl, newdata = .)
```

```{r}
mean(abs(act - pred)/act)
```

While both the methods offer similar final results, the bayesian optimiser completed its search in less than a minute where as the grid search took over seven minutes. Also, I find that I can use bayesian optimisation to search a larger parameter space more quickly than a traditional grid search. 

*Thoughts? Comments? Helpful? Not helpful? Like to see anything else added in here? Let me know!* 
